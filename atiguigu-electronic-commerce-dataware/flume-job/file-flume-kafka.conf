# 组件全局宏定义
collect.sources = r1
collect.channels=c1 c2
collect.sinks=k1 k2

# 配置 sources
collect.sources.r1.type=TAILDIR
## 记录采集日志到哪个偏移量的文件
collect.sources.r1.positionFile=/opt/bdd/apache-flume-1.7.0-bin/log_position.json
collect.sources.r1.filegroups=f1
## 要采集的日志所在目录
collect.sources.r1.filegroups.f1=/tmp/logs/app.*
collect.sources.r1.fileHeader=true



# 配置source端拦截器，type即拦截器的全类名
collect.sources.r1.interceptors=i1 i2
collect.sources.r1.interceptors.i1.type=org.mili.interceptors.LogETLInterceptor$Builder
collect.sources.r1.interceptors.i2.type=org.mili.interceptors.LogTypeInterceptor$Builder


# 配置selector
collect.sources.r1.selector.type=multiplexing
## logType是在拦截器中设置的header中的KV值中的K
collect.sources.r1.selector.header=logType
## start和event是logType对应的V,当V为start发送到c1,event发送到c2
collect.sources.r1.selector.mapping.start=c1
collect.sources.r1.selector.mapping.event=c2

# 配置channels
collect.channels.c1.type=memory
collect.channels.c1.capacity=10000
collect.channels.c1.byteCapacityBufferPercentage=20

collect.channels.c2.type=memory
collect.channels.c2.capacity=10000
collect.channels.c2.byteCapacityBufferPercentage=20

# 配置sinks

## 配置logType为start的sink
collect.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
### kafka中的topic，需要手动去kafka中创建
collect.sinks.k1.kafka.topic=topic_start
collect.sinks.k1.kafka.bootstrap.servers=node001:9092,node002:9092,node003:9092
collect.sinks.k1.kafka.flumeBatchSize=2000
collect.sinks.k1.kafka.producer.acks=1


## 配置logType为event的sink
collect.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink
### kafka中的topic，需要手动去kafka中创建
collect.sinks.k1.kafka.topic=topic_event
collect.sinks.k1.kafka.bootstrap.servers=node001:9092,node002:9092,node003:9092
collect.sinks.k1.kafka.flumeBatchSize=2000
collect.sinks.k1.kafka.producer.acks=1


# 配置各个组件之间的连接
collect.sources.r1.channels = c1 c2
collect.sinks.k1.channels=c1
collect.sinks.k1.channels=c2
